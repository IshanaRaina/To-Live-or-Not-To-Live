{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import sample\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    \"\"\"Take a csv file and return a Pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def downsample_data(df):\n",
    "    \"\"\"Downsample data in order to prevent effects of class imbalance problem.\"\"\"\n",
    "    majority_class_data = df[df['hospital_death'] == 0].reset_index(drop=True)\n",
    "    idx = list(range(0,83798))\n",
    "    random.seed(42)\n",
    "    subset = sample(idx, 7915)\n",
    "    majority_class_data = majority_class_data.loc[subset].reset_index(drop=True)\n",
    "    downsampled_data = majority_class_data.append(df[df['hospital_death'] == 1])\n",
    "    # Shuffling the dataset so the distribution of 0 and 1 is random throughout the dataset\n",
    "    downsampled_data = downsampled_data.sample(frac=1).reset_index(drop=True)\n",
    "    return downsampled_data\n",
    "\n",
    "def feature_target_split(df, target_variable):\n",
    "    \"\"\"Split the dataframe into feature variables and target variable.\"\"\"\n",
    "    feature_df = df.drop(target_variable, axis=1)\n",
    "    target = df[target_variable]\n",
    "    return feature_df, target\n",
    "    \n",
    "def drop_columns(df, var_list):\n",
    "    \"\"\"Clean dataframe column-wise by dealing with missing values.\"\"\"\n",
    "    df = df.drop(columns = var_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def drop_rows(df, idx):\n",
    "    \"\"\"Clean dataframe row-wise by dealing with missing values.\"\"\"\n",
    "    df = df.drop(df.index[idx]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def BMI_to_cat(x):\n",
    "    \"\"\"Transform BMI into a categorical variable.\"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x >= 0) and (x < 18.5) : return 'Underweight'\n",
    "        elif ((x >= 18.5 ) and (x <= 24.9)) : return 'Normal Weight'\n",
    "        elif ((x >= 25 )   and (x <= 29.9)) : return 'Overweight'\n",
    "        else : return 'Obese'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def PILD_to_cat(x):\n",
    "    \"\"\"Transform pre_icu_los_days into a categorical variable.\"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x <= 0)                : return 'Range[- to 0]'\n",
    "        elif (x > 0) and (x <= 5) : return  'Range[1 to 5]'\n",
    "        elif (x > 5) and (x <= 10) : return 'Range[5 to 10]'\n",
    "        elif (x > 10) and (x <= 15) : return 'Range[11 to 15]'\n",
    "        elif ((x > 15) and (x <= 20)) : return 'Range[16 to 20]'\n",
    "        elif ((x > 20) and (x <= 25)) : return 'Range[21 to 25]'\n",
    "        elif ((x > 25 ) and (x <= 30)) : return 'Range[25 to 30]'\n",
    "               \n",
    "        else : return 'Range[> 30]'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def num_cat_col(df, file):\n",
    "    \"\"\"Split the dataframe columns into categorical and numerical columns and return them as 2 separate lists.\"\"\"\n",
    "    num_col = []\n",
    "    cat_col = []\n",
    "    dictionary = load_file(file)\n",
    "    new_dictionary = dictionary[dictionary['Data Type'] == 'binary']\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].dtypes == 'object':\n",
    "            cat_col.append(column)\n",
    "        elif column in [value for key, value in new_dictionary['Variable Name'].iteritems()]:\n",
    "            cat_col.append(column)\n",
    "        else:\n",
    "            num_col.append(column)\n",
    "    return cat_col, num_col\n",
    "\n",
    "def impute_cat_col(df):\n",
    "    \"\"\"Impute null values in categorical variables.\"\"\"\n",
    "    # Imputing values for ethnicity\n",
    "    df.loc[:, 'ethnicity'] = df['ethnicity'].fillna('Other/Unknown')\n",
    "    \n",
    "    # Imputing values for gender and then encoding it as a numerical variable\n",
    "    np.random.seed(0)\n",
    "    gender_prob = df['gender'].value_counts(normalize=True).tolist()\n",
    "    df.loc[:, 'gender'] = df['gender'].fillna(pd.Series(np.random.choice(['M', 'F'], p=gender_prob, size=len(df))))\n",
    "    df.loc[:, 'gender'] = df['gender'].replace({'F': 0, 'M': 1})\n",
    "    \n",
    "    # Imputing missing values for icu_admit_source\n",
    "    df.loc[:, 'icu_admit_source'] = df['icu_admit_source'].fillna('Unknown')\n",
    "    \n",
    "    # Imputing missing values for apache_3j_bodysystem\n",
    "    df.loc[:, 'apache_3j_bodysystem'] = df['apache_3j_bodysystem'].fillna('Unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    \"\"\"One hot encode the categorical variables.\"\"\"\n",
    "    return pd.get_dummies(df)\n",
    "\n",
    "def concat_dfs(df1, df2):\n",
    "    \"\"\"Concatenate two dataframes.\"\"\"\n",
    "    return pd.concat([df1, df2], axis=1)\n",
    "\n",
    "def ordinal_temp(x):\n",
    "    \"\"\"Transform temperature related columns to an ordinal variable.\"\"\"\n",
    "    if x > 38:\n",
    "        return 3\n",
    "    elif x < 36:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def ordinal_heartrate(x):\n",
    "    \"\"\"Transform heart related columns to an ordinal variable.\"\"\"\n",
    "    if (x > 90) & (x < 100):\n",
    "        return 4\n",
    "    elif x >= 100:\n",
    "        return 5\n",
    "    elif (x >= 70) and (x <= 90):\n",
    "        return 3\n",
    "    elif (x >= 60) and (x < 70):\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_group_stats(df, col, target):\n",
    "    \"\"\"Add group description statistics to the dataframe.\"\"\"\n",
    "    groups = df.groupby(col)\n",
    "    group_stats = pd.DataFrame({'group_mean': groups[target].mean()})\n",
    "    group_stats['group_max'] = groups[target].max()\n",
    "    group_stats['group_min'] = groups[target].min()\n",
    "    group_stats['group_median'] = groups[target].median()\n",
    "    group_stats.reset_index(inplace=True)\n",
    "    return group_stats\n",
    "\n",
    "def impute_missing_val(X_train=None, X_test=None,  train_or_test='train'):\n",
    "    \"\"\"Use SimpleImputer to complete missing values.\"\"\"\n",
    "    if train_or_test == 'train':\n",
    "        imp = SimpleImputer(strategy=\"mean\")\n",
    "        X_imp = imp.fit_transform(X_train)\n",
    "        return X_imp\n",
    "    else:\n",
    "        imp = SimpleImputer(strategy='mean')\n",
    "        imp.fit(X_train)\n",
    "        X_imp = imp.transform(X_test)\n",
    "        return X_imp\n",
    "    \n",
    "def scale_data(X_train=None, X_test=None, train_or_test=\"train\"):\n",
    "    \"\"\"Use StandardScaler to scale values.\"\"\"\n",
    "    if train_or_test == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        X_sc = scaler.fit_transform(X_train)\n",
    "        return X_sc\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_sc = scaler.fit(X_train).transform(X_test)\n",
    "        return X_sc\n",
    "    \n",
    "def model_train_cv(model_def, X, y, cv=10, scoring='roc_auc'):\n",
    "    \"\"\"Fit the model, apply cross-validation and score the model.\"\"\"\n",
    "    model = model_def.fit(X,y)\n",
    "    score = cross_val_score(model_def, X, y, cv=cv, scoring=scoring).mean()\n",
    "    return model, score\n",
    "\n",
    "def model_predict_proba(model, X_to_predict):\n",
    "    \"\"\"Return the vector of predicted probabilities.\"\"\"\n",
    "    return model.predict_proba(X_to_predict)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing required files\n",
    "data_file = os.path.join(os.getcwd(), 'Datasets', 'training_v2.csv')\n",
    "dictionary_file = os.path.join(os.getcwd(), 'Datasets', 'WiDS Datathon 2020 Dictionary.csv')\n",
    "test_file = os.path.join(os.getcwd(), 'Datasets', 'unlabeled.csv')\n",
    "\n",
    "# Initializing required columns for feature engineering\n",
    "cat_cols_group_stats = ['bmi', 'ethnicity', 'icu_admit_source', 'icu_stay_type', \n",
    "                        'pre_icu_los_days', 'icu_type', 'apache_3j_bodysystem']\n",
    "\n",
    "temp_cols = ['temp_apache', 'd1_temp_max', 'd1_temp_min', 'h1_temp_max', 'h1_temp_min']\n",
    "new_temp_cols = ['temp_cat', 'd1_temp_max_cat', 'd1_temp_min_cat', 'h1_temp_max_cat', 'h1_temp_min_cat']\n",
    "\n",
    "heart_cols = ['h1_heartrate_max', 'h1_heartrate_min', 'd1_heartrate_max', 'd1_heartrate_min']\n",
    "new_heart_cols = ['h1_heartrate_max_cat', 'h1_heartrate_min_cat', 'd1_heartrate_max_cat', 'd1_heartrate_min_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training Data*************************\n",
      "\n",
      "Loading data...\n",
      "Training data shape: (91713, 186)\n",
      "\n",
      "Downsampling training data...\n",
      "Downsampled data shape: (15830, 186)\n",
      "\n",
      "Splitting training data into feature variables and target variable...\n",
      "Training feature data shape: (15830, 185)\n",
      "Training target data shape: (15830,)\n",
      "\n",
      "Dealing with missing values - COLUMN WISE...\n",
      "Dropping 47 columns\n",
      "Training feature data shape: (15830, 138)\n",
      "\n",
      "Dealing with missing values - ROW WISE...\n",
      "Dropping 121 rows\n",
      "Training feature data shape: (15709, 138)\n",
      "Training target data shape: (15709,)\n",
      "\n",
      "Transforming features...\n",
      "\n",
      "Splitting categorical variables and numerical variables...\n",
      "Categorical data shape: (15709, 24)\n",
      "Numerical data shape: (15709, 114)\n",
      "\n",
      "Cleaning up categorical variables...\n",
      "Dropping 3 columns\n",
      "Dropping 111 rows\n",
      "Categorical data shape: (15598, 21)\n",
      "\n",
      "Cleaning up numerical variables...\n",
      "Dropping 111 rows\n",
      "Dropping 32 columns\n",
      "Numerical data shape: (15598, 82)\n",
      "\n",
      "Cleaning up the target variable...\n",
      "Dropping 111 rows\n",
      "Training target shape: (15598,)\n",
      "\n",
      "Concatenating the numerical and categorical variables...\n",
      "New training feature dataset shape: (15598, 103)\n",
      "\n",
      "Transforming numerical features into ordinal features:\n",
      "\n",
      "Temperature features first...\n",
      "\n",
      "Heart rate features...\n",
      "\n",
      "Concatenating the training data and target variable...\n",
      "\n",
      "Creating a dataframe with group description statistics...\n",
      "\n",
      "Merging the group statistics with the training set...\n",
      "\n",
      "Removing the target column from the training set...\n",
      "\n",
      "One Hot Encoding the categorical variables...\n",
      "Categorical data shape: (15598, 156)\n",
      "\n",
      "Imputing missing values for the training set...\n",
      "\n",
      "Standardizing the training set...\n",
      "\n",
      "Training the model with the training set and generating a metric...\n",
      "Accuracy of the model with Cross Validation is: 0.9245532334003483\n",
      "\n",
      "Generating the probability predictions for the training set...\n",
      "[0.87507959 0.33701463 0.99597677 ... 0.12760123 0.0436439  0.65430135]\n",
      "\n",
      "\n",
      "*************************Test Data*************************\n",
      "\n",
      "Loading data...\n",
      "Test data shape: (39308, 186)\n",
      "\n",
      "Splitting test data into feature variables and target variable...\n",
      "Test feature data shape: (39308, 185)\n",
      "Test target data shape: (39308,)\n",
      "\n",
      "Dealing with missing values - COLUMN WISE...\n",
      "Dropping 47 columns\n",
      "Training feature data shape: (39308, 138)\n",
      "\n",
      "Transforming features...\n",
      "\n",
      "Splitting categorical variables and numerical variables...\n",
      "Categorical data shape: (39308, 24)\n",
      "Numerical data shape: (39308, 114)\n",
      "\n",
      "Cleaning up categorical variables...\n",
      "Dropping 3 columns\n",
      "Categorical data shape: (39308, 21)\n",
      "\n",
      "Cleaning up numerical variables...\n",
      "Dropping 32 columns\n",
      "Numerical data shape: (39308, 82)\n",
      "\n",
      "Concatenating the numerical and categorical variables...\n",
      "New training feature dataset shape: (39308, 103)\n",
      "\n",
      "Transforming numerical features into ordinal features\n",
      "\n",
      "Temperature features first\n",
      "\n",
      "Heart rate features\n",
      "\n",
      "Merging the group statistics from the training set with the testing set...\n",
      "\n",
      "One Hot Encoding the categorical variables...\n",
      "Test data shape: (39308, 156)\n",
      "\n",
      "Imputing missing values for the test set from the training set ...\n",
      "\n",
      "Standardizing the test set from the training set...\n",
      "\n",
      "Generating the probability predictions for the test set...\n",
      "[0.09761066 0.13665527 0.21563226 ... 0.88834524 0.14118093 0.99447792]\n"
     ]
    }
   ],
   "source": [
    "print('*************************Training Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "data = load_file(data_file)\n",
    "print('Training data shape: {}'.format(data.shape))\n",
    "\n",
    "print('\\nDownsampling training data...')\n",
    "new_data = downsample_data(data)\n",
    "print('Downsampled data shape: {}'.format(new_data.shape))\n",
    "\n",
    "print('\\nSplitting training data into feature variables and target variable...')\n",
    "train_X, train_y = feature_target_split(new_data, 'hospital_death')\n",
    "print('Training feature data shape: {}'.format(train_X.shape))\n",
    "print('Training target data shape: {}'.format(train_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "# Columns to be dropped based on manual evaluation\n",
    "feature_list = ['encounter_id', 'patient_id', 'hospital_id', 'icu_id', 'height', 'weight', 'readmission_status']\n",
    "# Columns to be dropped that have greater than 75% of their data missing\n",
    "columns = train_X.columns[train_X.isnull().mean() > 0.75].tolist()\n",
    "for column in columns:\n",
    "    feature_list.append(column)\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "column_majority_train_X = drop_columns(train_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(column_majority_train_X.shape))\n",
    "\n",
    "print('\\nDealing with missing values - ROW WISE...')\n",
    "# Rows to be dropped that have greater than 50% of their data missing\n",
    "idx1 = column_majority_train_X.index[column_majority_train_X.isnull().mean(axis=1) > 0.5].tolist()\n",
    "print('Dropping {} rows'.format(len(idx1)))\n",
    "clean_train_X = drop_rows(column_majority_train_X, idx1)\n",
    "print('Training feature data shape: {}'.format(clean_train_X.shape))\n",
    "clean_train_y = drop_rows(train_y, idx1)\n",
    "print('Training target data shape: {}'.format(clean_train_y.shape))\n",
    "\n",
    "print('\\nTransforming features...')\n",
    "clean_train_X['bmi'] = clean_train_X['bmi'].apply(BMI_to_cat)\n",
    "clean_train_X['pre_icu_los_days'] = clean_train_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "cat_col, num_col = num_cat_col(clean_train_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_train_X[cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_train_X[num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "cat_list = ['hospital_admit_source', 'apache_2_bodysystem', 'gcs_unable_apache']\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_cat = drop_columns(clean_train_X[cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_cat_imputed = impute_cat_col(X_cat)\n",
    "# Obtaining the index values of rows where arf_apache is null and then dropping them\n",
    "idx2 = X_cat_imputed.index[X_cat_imputed['arf_apache'].isnull()].tolist()\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_cat_final = drop_rows(X_cat_imputed, idx2)\n",
    "print('Categorical data shape: {}'.format(X_cat_final.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_num = drop_rows(clean_train_X[num_col], idx2)\n",
    "num_column_list = X_num.columns[X_num.isnull().mean() > 0.50].tolist()\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_num = drop_columns(X_num, num_column_list)\n",
    "print('Numerical data shape: {}'.format(X_num.shape))\n",
    "\n",
    "print('\\nCleaning up the target variable...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "final_train_y = drop_rows(clean_train_y, idx2)\n",
    "print('Training target shape: {}'.format(final_train_y.shape))\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "new_dataset = concat_dfs(X_num, X_cat_final)\n",
    "print('New training feature dataset shape: {}'.format(new_dataset.shape))\n",
    "\n",
    "print('\\nTransforming numerical features into ordinal features:')\n",
    "print('\\nTemperature features first...')\n",
    "for i in range(len(temp_cols)):\n",
    "    new_dataset[new_temp_cols[i]] = new_dataset[temp_cols[i]].apply(ordinal_temp)\n",
    "\n",
    "print('\\nHeart rate features...')\n",
    "for i in range(len(heart_cols)):\n",
    "    new_dataset[new_heart_cols[i]] = new_dataset[heart_cols[i]].apply(ordinal_heartrate)\n",
    "\n",
    "print('\\nConcatenating the training data and target variable...')\n",
    "new_dataset = concat_dfs(final_train_y, new_dataset)\n",
    "\n",
    "print('\\nCreating a dataframe with group description statistics...')\n",
    "group_stats = get_group_stats(new_dataset, cat_cols_group_stats, 'hospital_death')\n",
    "\n",
    "print('\\nMerging the group statistics with the training set...')\n",
    "tmp_df = pd.merge(new_dataset, group_stats, on=cat_cols_group_stats, how='left')\n",
    "\n",
    "print('\\nRemoving the target column from the training set...')\n",
    "tmp = drop_columns(tmp_df, 'hospital_death')\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_final = one_hot_encode(tmp)\n",
    "print('Categorical data shape: {}'.format(X_final.shape))\n",
    "\n",
    "print('\\nImputing missing values for the training set...')\n",
    "X_final_imp = impute_missing_val(X_final, None, 'train')\n",
    "\n",
    "print('\\nStandardizing the training set...')\n",
    "X_final_imp_sc = scale_data(X_final_imp, None, 'train')\n",
    "\n",
    "print('\\nTraining the model with the training set and generating a metric...')\n",
    "# Gradient Boosting Classifier\n",
    "model, score = model_train_cv(GradientBoostingClassifier(random_state=42, \n",
    "                                                         n_estimators=300,\n",
    "                                                         max_features='auto', \n",
    "                                                         max_depth=5, \n",
    "                                                         subsample=1),\n",
    "                              X_final_imp_sc, \n",
    "                              final_train_y)\n",
    "print(\"Accuracy of the model with Cross Validation is:\", score)\n",
    "\n",
    "print('\\nGenerating the probability predictions for the training set...')\n",
    "y_pred_train = model_predict_proba(model, X_final_imp_sc)\n",
    "print(y_pred_train)\n",
    "\n",
    "print('\\n\\n*************************Test Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "test_data = load_file(test_file)\n",
    "print('Test data shape: {}'.format(test_data.shape))\n",
    "\n",
    "print('\\nSplitting test data into feature variables and target variable...')\n",
    "test_X, test_y = feature_target_split(test_data, 'hospital_death')\n",
    "print('Test feature data shape: {}'.format(test_X.shape))\n",
    "print('Test target data shape: {}'.format(test_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "clean_test_X = drop_columns(test_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(clean_test_X.shape))\n",
    "\n",
    "print('\\nTransforming features...')\n",
    "clean_test_X['bmi'] = clean_test_X['bmi'].apply(BMI_to_cat)\n",
    "clean_test_X['pre_icu_los_days'] = clean_test_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "test_cat_col, test_num_col = num_cat_col(clean_test_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_test_X[test_cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_test_X[test_num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_test_cat = drop_columns(clean_test_X[test_cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_test_cat_imputed = impute_cat_col(X_test_cat)\n",
    "# Imputing values instead of dropping rows\n",
    "np.random.seed(0)\n",
    "my_list = ['arf_apache', 'intubated_apache', 'ventilated_apache', 'aids', 'cirrhosis', \n",
    "            'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', \n",
    "            'solid_tumor_with_metastasis']\n",
    "for my_column in my_list:\n",
    "    keys_ = X_test_cat_imputed[my_column].value_counts().keys().tolist()\n",
    "    prob = X_test_cat_imputed[my_column].value_counts(normalize=True).tolist()\n",
    "    X_test_cat_imputed.loc[:, my_column] = X_test_cat_imputed[my_column].fillna(\n",
    "                                           pd.Series(np.random.choice(keys_, p=prob, size=len(X_test_cat_imputed))))\n",
    "print('Categorical data shape: {}'.format(X_test_cat_imputed.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_test_num = drop_columns(clean_test_X[test_num_col], num_column_list)\n",
    "print('Numerical data shape: {}'.format(X_test_num.shape))\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "test_dataset = concat_dfs(X_test_num, X_test_cat_imputed)\n",
    "print('New training feature dataset shape: {}'.format(test_dataset.shape))\n",
    "\n",
    "print('\\nTransforming numerical features into ordinal features')\n",
    "print('\\nTemperature features first')\n",
    "for i in range(len(temp_cols)):\n",
    "    test_dataset[new_temp_cols[i]] = test_dataset[temp_cols[i]].apply(ordinal_temp)\n",
    "\n",
    "print('\\nHeart rate features')\n",
    "for i in range(len(heart_cols)):\n",
    "    test_dataset[new_heart_cols[i]] = test_dataset[heart_cols[i]].apply(ordinal_heartrate)\n",
    "\n",
    "print('\\nMerging the group statistics from the training set with the testing set...')\n",
    "test_df = pd.merge(test_dataset, group_stats, on=cat_cols_group_stats, how='left')\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_test_final = one_hot_encode(test_df)\n",
    "print('Test data shape: {}'.format(X_test_final.shape))\n",
    "\n",
    "print('\\nImputing missing values for the test set from the training set ...')\n",
    "X_test_final = impute_missing_val(X_final, X_test_final, 'test')\n",
    "\n",
    "print('\\nStandardizing the test set from the training set...')\n",
    "X_test_final = scale_data(X_final_imp, X_test_final, 'test')\n",
    "\n",
    "print('\\nGenerating the probability predictions for the test set...')\n",
    "y_pred = model_predict_proba(model, X_test_final)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the submission file to submit to Kaggle\n",
    "submission = pd.DataFrame(test_data['encounter_id'])\n",
    "submission['hospital_death'] = pd.Series(y_pred)\n",
    "submission = submission.iloc[:39308,]\n",
    "submission.to_csv('submission_file.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
