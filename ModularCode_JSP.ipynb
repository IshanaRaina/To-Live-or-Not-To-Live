{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    \"\"\"\n",
    "    Takes a csv file and returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def downsample_data(df):\n",
    "    \"\"\"\n",
    "    Downsamples data in order to prevent effects of class imbalance problem.\n",
    "    \"\"\"\n",
    "    majority_class_data = df[df['hospital_death'] == 0].reset_index(drop=True)\n",
    "    idx = list(range(0,83798))\n",
    "    random.seed(42)\n",
    "    subset = sample(idx, 7915)\n",
    "    majority_class_data = majority_class_data.loc[subset].reset_index(drop=True)\n",
    "    downsampled_data = majority_class_data.append(df[df['hospital_death'] == 1])\n",
    "    # Shuffling the dataset so the distribution of 0 and 1 is random throughout the dataset\n",
    "    downsampled_data = downsampled_data.sample(frac=1).reset_index(drop=True)\n",
    "    return downsampled_data\n",
    "\n",
    "def feature_target_split(df, target_variable):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into feature variables and target variable.\n",
    "    \"\"\"\n",
    "    feature_df = df.drop(target_variable, axis=1)\n",
    "    target = df[target_variable]\n",
    "    return feature_df, target\n",
    "    \n",
    "def drop_columns(df, var_list):\n",
    "    \"\"\"\n",
    "    Cleans dataframe column-wise by dealing with missing values.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns = var_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def drop_rows(df, idx):\n",
    "    \"\"\"\n",
    "    Cleans dataframe row-wise by dealing with missing values.\n",
    "    \"\"\"\n",
    "    df = df.drop(df.index[idx]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def BMI_to_cat(x):\n",
    "    \"\"\"\n",
    "    Transforms BMI into a categorical variable.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x >= 0) and (x < 18.5) : return 'Underweight'\n",
    "        elif ((x >= 18.5 ) and (x <= 24.9)) : return 'Normal Weight'\n",
    "        elif ((x >= 25 )   and (x <= 29.9)) : return 'Overweight'\n",
    "        else : return 'Obese'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def PILD_to_cat(x):\n",
    "    \"\"\"\n",
    "    Transforms pre_icu_los_days into a categorical variable.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x <= 0)                : return 'Range[- to 0]'\n",
    "        elif (x > 0) and (x <= 5) : return  'Range[1 to 5]'\n",
    "        elif (x > 5) and (x <= 10) : return 'Range[5 to 10]'\n",
    "        elif (x > 10) and (x <= 15) : return 'Range[11 to 15]'\n",
    "        elif ((x > 15) and (x <= 20)) : return 'Range[16 to 20]'\n",
    "        elif ((x > 20) and (x <= 25)) : return 'Range[21 to 25]'\n",
    "        elif ((x > 25 ) and (x <= 30)) : return 'Range[25 to 30]'\n",
    "               \n",
    "        else : return 'Range[> 30]'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def num_cat_col(df, file):\n",
    "    \"\"\"\n",
    "    Splits the columns of the dataframe into categorical and numerical columns \n",
    "    and returns them as 2 separate lists.\n",
    "    Columns having binary values (0 or 1) are added to the list of categorical columns.\n",
    "    \"\"\"\n",
    "    num_col = []\n",
    "    cat_col = []\n",
    "    dictionary = load_file(file)\n",
    "    new_dictionary = dictionary[dictionary['Data Type'] == 'binary']\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].dtypes == 'object':\n",
    "            cat_col.append(column)\n",
    "        elif column in [value for key, value in new_dictionary['Variable Name'].iteritems()]:\n",
    "            cat_col.append(column)\n",
    "        else:\n",
    "            num_col.append(column)\n",
    "    return cat_col, num_col\n",
    "\n",
    "def impute_cat_col(df):\n",
    "    \"\"\"\n",
    "    Imputes null values in categorical variables.\n",
    "    \"\"\"\n",
    "    # Imputing values for ethnicity\n",
    "    df.loc[:, 'ethnicity'] = df['ethnicity'].fillna('Other/Unknown')\n",
    "    \n",
    "    # Imputing values for gender and then encoding it as a numerical variable\n",
    "    np.random.seed(0)\n",
    "    gender_prob = df['gender'].value_counts(normalize=True).tolist()\n",
    "    df.loc[:, 'gender'] = df['gender'].fillna(pd.Series(np.random.choice(['M', 'F'], p=gender_prob, size=len(df))))\n",
    "    df.loc[:, 'gender'] = df['gender'].replace({'F': 0, 'M': 1})\n",
    "    \n",
    "    # Imputing missing values for icu_admit_source\n",
    "    df.loc[:, 'icu_admit_source'] = df['icu_admit_source'].fillna('Unknown')\n",
    "    \n",
    "    # Imputing missing values for apache_3j_bodysystem\n",
    "    df.loc[:, 'apache_3j_bodysystem'] = df['apache_3j_bodysystem'].fillna('Unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_num_col(df):\n",
    "    \"\"\"\n",
    "    Imputes null values in numerical variables with the mean.\n",
    "    \"\"\"\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    \"\"\"\n",
    "    One hot encoding the categorical variables.\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(df)\n",
    "\n",
    "def concat_num_cat(df1, df2):\n",
    "    \"\"\"\n",
    "    Concatenating the numerical and categorical columns to get the final consolidated dataset.\n",
    "    \"\"\"\n",
    "    return pd.concat([df1, df2], axis=1)\n",
    "\n",
    "def ordinal_temp(x):\n",
    "    if x > 38:\n",
    "        return 3\n",
    "    elif x < 36:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def ordinal_heartrate(x):\n",
    "    if (x > 90) & (x < 100):\n",
    "        return 4\n",
    "    elif x >= 100:\n",
    "        return 5\n",
    "    elif (x >= 70) and (x <= 90):\n",
    "        return 3\n",
    "    elif (x >= 60) and (x < 70):\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_group_stats(df, col, target):\n",
    "    groups = df.groupby(col)\n",
    "    group_stats = pd.DataFrame({'group_mean': groups[target].mean()})\n",
    "    group_stats['group_max'] = groups[target].max()\n",
    "    group_stats['group_min'] = groups[target].min()\n",
    "    group_stats['group_median'] = groups[target].median()\n",
    "    group_stats.reset_index(inplace=True)\n",
    "    return group_stats\n",
    "\n",
    "\n",
    "def imputeMissingVal(X_train=None, X_test=None,  train_or_test='train'):\n",
    "    if train_or_test == 'train':\n",
    "        imp = Imputer(strategy=\"mean\")\n",
    "        X_imp = imp.fit_transform(X_train)\n",
    "        return X_imp\n",
    "    else:\n",
    "        imp = Imputer(strategy='mean')\n",
    "        X_obj = imp.fit(X_train)\n",
    "        X_imp = imp.transform(X_test)\n",
    "        return X_imp\n",
    "    \n",
    "def scaleData(X_train=None, X_test=None, train_or_test=\"train\"):\n",
    "    if train_or_test == 'train':\n",
    "        scaler = StandardScaler()\n",
    "        X_sc = scaler.fit_transform(X_train)\n",
    "        return X_sc\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_sc = scaler.fit(X_train).transform(X_test)\n",
    "        return X_sc\n",
    "    \n",
    "def model_train_cv(model_def, X, y, cv=5, scoring='roc_auc'):\n",
    "    model = model_def.fit(X,y)\n",
    "    score = cross_val_score(model, X, y, cv=cv, scoring=scoring).mean()\n",
    "    return model, score\n",
    "\n",
    "def model_predictProba(model, X_ToPredict):\n",
    "    return model.predict_proba(X_ToPredict)[:,1]\n",
    "\n",
    "    \n",
    "cat_cols_group_stats = ['bmi', 'ethnicity', 'icu_admit_source', 'icu_stay_type', \n",
    "           'pre_icu_los_days', 'icu_type', 'apache_3j_bodysystem']\n",
    "\n",
    "temp_cols = ['temp_apache', 'd1_temp_max', 'd1_temp_min', 'h1_temp_max', 'h1_temp_min']\n",
    "new_temp_cols = ['temp_cat', 'd1_temp_max_cat', 'd1_temp_min_cat', 'h1_temp_max_cat', 'h1_temp_min_cat']\n",
    "\n",
    "heart_cols = ['h1_heartrate_max', 'h1_heartrate_min', 'd1_heartrate_max', 'd1_heartrate_min']\n",
    "new_heart_cols = ['h1_heartrate_max_cat', 'h1_heartrate_min_cat', 'd1_heartrate_max_cat', 'd1_heartrate_min_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required files\n",
    "data_file = 'training_v2.csv'\n",
    "dictionary_file = 'WiDS Datathon 2020 Dictionary.csv'\n",
    "test_file = 'unlabeled.csv'\n",
    "\n",
    "print('*************************Training Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "data = load_file(data_file)\n",
    "print('Training data shape: {}'.format(data.shape))\n",
    "\n",
    "print('\\nDownsampling training data...')\n",
    "new_data = downsample_data(data)\n",
    "print('Downsampled data shape: {}'.format(new_data.shape))\n",
    "\n",
    "print('\\nSplitting training data into feature variables and target variable...')\n",
    "train_X, train_y = feature_target_split(new_data, 'hospital_death')\n",
    "print('Training feature data shape: {}'.format(train_X.shape))\n",
    "print('Training target data shape: {}'.format(train_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "# Columns to be dropped based on manual evaluation\n",
    "feature_list = ['encounter_id', 'patient_id', 'hospital_id', 'icu_id', 'height', 'weight', 'readmission_status']\n",
    "# Columns to be dropped that have greater than 75% of their data missing\n",
    "columns = train_X.columns[train_X.isnull().mean() > 0.75].tolist()\n",
    "for column in columns:\n",
    "    feature_list.append(column)\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "column_majority_train_X = drop_columns(train_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(column_majority_train_X.shape))\n",
    "\n",
    "print('\\nDealing with missing values - ROW WISE...')\n",
    "# Rows to be dropped that have greater than 50% of their data missing\n",
    "idx1 = column_majority_train_X.index[column_majority_train_X.isnull().mean(axis=1) > 0.5].tolist()\n",
    "print('Dropping {} rows'.format(len(idx1)))\n",
    "clean_train_X = drop_rows(column_majority_train_X, idx1)\n",
    "print('Training feature data shape: {}'.format(clean_train_X.shape))\n",
    "clean_train_y = drop_rows(train_y, idx1)\n",
    "print('Training target data shape: {}'.format(clean_train_y.shape))\n",
    "\n",
    "print('\\nTransforming features...')\n",
    "clean_train_X['bmi'] = clean_train_X['bmi'].apply(BMI_to_cat)\n",
    "clean_train_X['pre_icu_los_days'] = clean_train_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "cat_col, num_col = num_cat_col(clean_train_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_train_X[cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_train_X[num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "cat_list = ['hospital_admit_source', 'apache_2_bodysystem', 'gcs_unable_apache']\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_cat = drop_columns(clean_train_X[cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_cat_imputed = impute_cat_col(X_cat)\n",
    "# Obtaining the index values of rows where arf_apache is null and then dropping them\n",
    "idx2 = X_cat_imputed.index[X_cat_imputed['arf_apache'].isnull()].tolist()\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_cat_final = drop_rows(X_cat_imputed, idx2)\n",
    "print('Categorical data shape: {}'.format(X_cat_final.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_num = drop_rows(clean_train_X[num_col], idx2)\n",
    "num_column_list = X_num.columns[X_num.isnull().mean() > 0.50].tolist()\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_num = drop_columns(X_num, num_column_list)\n",
    "X_num = impute_num_col(X_num)\n",
    "print('Numerical data shape: {}'.format(X_num.shape))\n",
    "\n",
    "print('\\nCleaning up the target variable...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "final_train_y = drop_rows(clean_train_y, idx2)\n",
    "print('Training target shape: {}'.format(final_train_y.shape))\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "new_dataset = concat_num_cat(X_num, X_cat_final)\n",
    "print('New training feature dataset shape: {}'.format(new_dataset.shape))\n",
    "\n",
    "print('\\nTransforming categorical features into ordinal features')\n",
    "print('\\nTemperature features first')\n",
    "for i in range(len(temp_cols)):\n",
    "    new_dataset[new_temp_cols[i]] = new_dataset[temp_cols[i]].apply(ordinal_temp)\n",
    "\n",
    "print('\\nHeart rate features')\n",
    "for i in range(len(heart_cols)):\n",
    "    new_dataset[new_heart_cols[i]] = new_dataset[heart_cols[i]].apply(ordinal_heartrate)\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "new_dataset = concat_num_cat(final_train_y, new_dataset)\n",
    "\n",
    "print('\\nCreating a dataframe with group description statisitics...')\n",
    "group_stats = get_group_stats(new_dataset, cat_cols_group_stats, 'hospital_death')\n",
    "\n",
    "print('\\nMerging the group statistics with the training set...')\n",
    "tmp_df = pd.merge(new_dataset, group_stats, on=cat_cols_group_stats, how='left')\n",
    "\n",
    "print('\\nRemoving the target column from the training set...')\n",
    "tmp = drop_columns(tmp_df, 'hospital_death')\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_final = one_hot_encode(tmp)\n",
    "print('Categorcal data shape: {}'.format(X_final.shape))\n",
    "\n",
    "print('\\nImputing missing value for the training set...')\n",
    "X_final_imp = imputeMissingVal(X_final, None, 'train')\n",
    "\n",
    "print('\\nStandardizing the training set...')\n",
    "X_final_imp_sc = scaleData(X_final_imp, None, 'train')\n",
    "\n",
    "print('\\nTraining the model with the training set and generating a metric...')\n",
    "gbm_model, score = model_train_cv(GradientBoostingClassifier(random_state=42, n_estimators=200,\n",
    "                                   max_features='auto' , max_depth=5, subsample=0.8),\n",
    "                 X_final_imp_sc, final_train_y)\n",
    "\n",
    "print(\"Cross validation Score:\", score)\n",
    "y_pred_train = model_predictProba(gbm_model, X_final_imp_sc)\n",
    "print(y_pred_train)\n",
    "\n",
    "print('\\n\\n*************************Test Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "test_data = load_file(test_file)\n",
    "print('Test data shape: {}'.format(test_data.shape))\n",
    "\n",
    "print('\\nSplitting test data into feature variables and target variable...')\n",
    "test_X, test_y = feature_target_split(test_data, 'hospital_death')\n",
    "print('Test feature data shape: {}'.format(test_X.shape))\n",
    "print('Test target data shape: {}'.format(test_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "clean_test_X = drop_columns(test_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(clean_test_X.shape))\n",
    "\n",
    "# print('\\nTransforming features...')\n",
    "clean_test_X['bmi'] = clean_test_X['bmi'].apply(BMI_to_cat)\n",
    "clean_test_X['pre_icu_los_days'] = clean_test_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "test_cat_col, test_num_col = num_cat_col(clean_test_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_test_X[test_cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_test_X[test_num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_test_cat = drop_columns(clean_test_X[test_cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_test_cat_imputed = impute_cat_col(X_test_cat)\n",
    "\n",
    "# Imputing values instead of dropping rows\n",
    "np.random.seed(0)\n",
    "my_list = ['arf_apache', 'intubated_apache', 'ventilated_apache', 'aids', 'cirrhosis', \n",
    "            'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', \n",
    "            'solid_tumor_with_metastasis']\n",
    "for my_column in my_list:\n",
    "    keys_list = X_test_cat_imputed[my_column].value_counts().keys().tolist()\n",
    "    prob = X_test_cat_imputed[my_column].value_counts(normalize=True).tolist()\n",
    "    X_test_cat_imputed.loc[:, my_column] = X_test_cat_imputed[my_column].fillna(\n",
    "        pd.Series(np.random.choice(keys_list, p=prob, size=len(X_test_cat_imputed))))\n",
    "    \n",
    "print('Categorical data shape: {}'.format(X_test_cat_imputed.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_test_num = drop_columns(clean_test_X[test_num_col], num_column_list)\n",
    "X_test_num = impute_num_col(X_test_num)\n",
    "print('Numerical data shape: {}'.format(X_test_num.shape))\n",
    "\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "test_dataset = concat_num_cat(X_test_num, X_test_cat_imputed)\n",
    "print('New training feature dataset shape: {}'.format(test_dataset.shape))\n",
    "\n",
    "print('\\nTransforming categorical features into ordinal features')\n",
    "print('\\nTemperature features first')\n",
    "for i in range(len(temp_cols)):\n",
    "    test_dataset[new_temp_cols[i]] = test_dataset[temp_cols[i]].apply(ordinal_temp)\n",
    "\n",
    "print('\\nHeart rate features')\n",
    "for i in range(len(heart_cols)):\n",
    "    test_dataset[new_heart_cols[i]] = test_dataset[heart_cols[i]].apply(ordinal_heartrate)\n",
    "\n",
    "print('\\nMerging the group statistics from the training set with the testing set...')\n",
    "test_df = pd.merge(test_dataset, group_stats, on=cat_cols_group_stats, how='left')\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_test_final = one_hot_encode(test_df)\n",
    "print('Test data shape: {}'.format(X_test_final.shape))\n",
    "\n",
    "print('\\nImputing missing value from the training set ...')\n",
    "X_test_final = imputeMissingVal(X_final, X_test_final, 'test')\n",
    "\n",
    "print('\\nStandardizing from the training set...')\n",
    "X_test_final = scaleData(X_final_imp, X_test_final, 'test')\n",
    "\n",
    "print('\\nGenerating the probability predictions for the test set...')\n",
    "y_pred = model_predictProba(gbm_model, X_test_final)\n",
    "print('y_pred:', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
