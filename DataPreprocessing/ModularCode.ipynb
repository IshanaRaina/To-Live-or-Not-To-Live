{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "pd.set_option('display.max_columns', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    \"\"\"\n",
    "    Takes a csv file and returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def downsample_data(df):\n",
    "    \"\"\"\n",
    "    Downsamples data in order to prevent effects of class imbalance problem.\n",
    "    \"\"\"\n",
    "    majority_class_data = df[df['hospital_death'] == 0].reset_index(drop=True)\n",
    "    idx = list(range(0,83798))\n",
    "    random.seed(42)\n",
    "    subset = sample(idx, 7915)\n",
    "    majority_class_data = majority_class_data.loc[subset].reset_index(drop=True)\n",
    "    downsampled_data = majority_class_data.append(df[df['hospital_death'] == 1])\n",
    "    # Shuffling the dataset so the distribution of 0 and 1 is random throughout the dataset\n",
    "    downsampled_data = downsampled_data.sample(frac=1).reset_index(drop=True)\n",
    "    return downsampled_data\n",
    "\n",
    "def feature_target_split(df, target_variable):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into feature variables and target variable.\n",
    "    \"\"\"\n",
    "    feature_df = df.drop(target_variable, axis=1)\n",
    "    target = df[target_variable]\n",
    "    return feature_df, target\n",
    "    \n",
    "def drop_columns(df, var_list):\n",
    "    \"\"\"\n",
    "    Cleans dataframe column-wise by dealing with missing values.\n",
    "    \"\"\"\n",
    "    df = df.drop(columns = var_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def drop_rows(df, idx):\n",
    "    \"\"\"\n",
    "    Cleans dataframe row-wise by dealing with missing values.\n",
    "    \"\"\"\n",
    "    df = df.drop(df.index[idx]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def BMI_to_cat(x):\n",
    "    \"\"\"\n",
    "    Transforms BMI into a categorical variable.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x >= 0) and (x < 18.5) : return 'Underweight'\n",
    "        elif ((x >= 18.5 ) and (x <= 24.9)) : return 'Normal Weight'\n",
    "        elif ((x >= 25 )   and (x <= 29.9)) : return 'Overweight'\n",
    "        else : return 'Obese'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def PILD_to_cat(x):\n",
    "    \"\"\"\n",
    "    Transforms pre_icu_los_days into a categorical variable.\n",
    "    \"\"\"\n",
    "    try :\n",
    "        x = float(x)\n",
    "        if (x <= 0)                : return 'Range[- to 0]'\n",
    "        elif (x > 0) and (x <= 5) : return  'Range[1 to 5]'\n",
    "        elif (x > 5) and (x <= 10) : return 'Range[5 to 10]'\n",
    "        elif (x > 10) and (x <= 15) : return 'Range[11 to 15]'\n",
    "        elif ((x > 15) and (x <= 20)) : return 'Range[16 to 20]'\n",
    "        elif ((x > 20) and (x <= 25)) : return 'Range[21 to 25]'\n",
    "        elif ((x > 25 ) and (x <= 30)) : return 'Range[25 to 30]'\n",
    "               \n",
    "        else : return 'Range[> 30]'\n",
    "    except ValueError:\n",
    "        return 'Other'\n",
    "    \n",
    "def num_cat_col(df, file):\n",
    "    \"\"\"\n",
    "    Splits the columns of the dataframe into categorical and numerical columns \n",
    "    and returns them as 2 separate lists.\n",
    "    Columns having binary values (0 or 1) are added to the list of categorical columns.\n",
    "    \"\"\"\n",
    "    num_col = []\n",
    "    cat_col = []\n",
    "    dictionary = load_file(file)\n",
    "    new_dictionary = dictionary[dictionary['Data Type'] == 'binary']\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].dtypes == 'object':\n",
    "            cat_col.append(column)\n",
    "        elif column in [value for key, value in new_dictionary['Variable Name'].iteritems()]:\n",
    "            cat_col.append(column)\n",
    "        else:\n",
    "            num_col.append(column)\n",
    "    return cat_col, num_col\n",
    "\n",
    "def impute_cat_col(df):\n",
    "    \"\"\"\n",
    "    Imputes null values in categorical variables.\n",
    "    \"\"\"\n",
    "    # Imputing values for ethnicity\n",
    "    df.loc[:, 'ethnicity'] = df['ethnicity'].fillna('Other/Unknown')\n",
    "    \n",
    "    # Imputing values for gender and then encoding it as a numerical variable\n",
    "    np.random.seed(0)\n",
    "    gender_prob = df['gender'].value_counts(normalize=True).tolist()\n",
    "    df.loc[:, 'gender'] = df['gender'].fillna(pd.Series(np.random.choice(['M', 'F'], p=gender_prob, size=len(df))))\n",
    "    df.loc[:, 'gender'] = df['gender'].replace({'F': 0, 'M': 1})\n",
    "    \n",
    "    # Imputing missing values for icu_admit_source\n",
    "    df.loc[:, 'icu_admit_source'] = df['icu_admit_source'].fillna('Unknown')\n",
    "    \n",
    "    # Imputing missing values for apache_3j_bodysystem\n",
    "    df.loc[:, 'apache_3j_bodysystem'] = df['apache_3j_bodysystem'].fillna('Unknown')\n",
    "\n",
    "    return df\n",
    "\n",
    "def impute_num_col(df):\n",
    "    \"\"\"\n",
    "    Imputes null values in numerical variables with the mean.\n",
    "    \"\"\"\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "def one_hot_encode(df):\n",
    "    \"\"\"\n",
    "    One hot encoding the categorical variables.\n",
    "    \"\"\"\n",
    "    return pd.get_dummies(df)\n",
    "\n",
    "def concat_num_cat(df1, df2):\n",
    "    \"\"\"\n",
    "    Concatenating the numerical and categorical columns to get the final consolidated dataset.\n",
    "    \"\"\"\n",
    "    return pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************Training Data*************************\n",
      "\n",
      "Loading data...\n",
      "Training data shape: (91713, 186)\n",
      "\n",
      "Downsampling training data...\n",
      "Downsampled data shape: (15830, 186)\n",
      "\n",
      "Splitting training data into feature variables and target variable...\n",
      "Training feature data shape: (15830, 185)\n",
      "Training target data shape: (15830,)\n",
      "\n",
      "Dealing with missing values - COLUMN WISE...\n",
      "Dropping 47 columns\n",
      "Training feature data shape: (15830, 138)\n",
      "\n",
      "Dealing with missing values - ROW WISE...\n",
      "Dropping 121 rows\n",
      "Training feature data shape: (15709, 138)\n",
      "Training target data shape: (15709,)\n",
      "\n",
      "Transforming features...\n",
      "\n",
      "Splitting categorical variables and numerical variables...\n",
      "Categorical data shape: (15709, 24)\n",
      "Numerical data shape: (15709, 114)\n",
      "\n",
      "Cleaning up categorical variables...\n",
      "Dropping 3 columns\n",
      "Dropping 111 rows\n",
      "Categorical data shape: (15598, 21)\n",
      "\n",
      "Cleaning up numerical variables...\n",
      "Dropping 111 rows\n",
      "Dropping 32 columns\n",
      "Numerical data shape: (15598, 82)\n",
      "\n",
      "Cleaning up the target variable...\n",
      "Dropping 111 rows\n",
      "Training target shape: (15598,)\n",
      "\n",
      "One Hot Encoding the categorical variables...\n",
      "Categorcal data shape: (15598, 61)\n",
      "\n",
      "Concatenating the numerical and categorical variables...\n",
      "New training feature dataset shape: (15598, 143)\n",
      "\n",
      "\n",
      "*************************Test Data*************************\n",
      "\n",
      "Loading data...\n",
      "Test data shape: (39308, 186)\n",
      "\n",
      "Splitting test data into feature variables and target variable...\n",
      "Test feature data shape: (39308, 185)\n",
      "Test target data shape: (39308,)\n",
      "\n",
      "Dealing with missing values - COLUMN WISE...\n",
      "Dropping 47 columns\n",
      "Training feature data shape: (39308, 138)\n",
      "\n",
      "Transforming features...\n",
      "\n",
      "Splitting categorical variables and numerical variables...\n",
      "Categorical data shape: (39308, 24)\n",
      "Numerical data shape: (39308, 114)\n",
      "\n",
      "Cleaning up categorical variables...\n",
      "Dropping 3 columns\n",
      "Categorical data shape: (39308, 21)\n",
      "\n",
      "Cleaning up numerical variables...\n",
      "Dropping 32 columns\n",
      "Numerical data shape: (39308, 82)\n",
      "\n",
      "One Hot Encoding the categorical variables...\n",
      "Categorcal data shape: (39308, 61)\n",
      "\n",
      "Concatenating the numerical and categorical variables...\n",
      "New training feature dataset shape: (39308, 143)\n"
     ]
    }
   ],
   "source": [
    "# Define required files\n",
    "data_file = 'training_v2.csv'\n",
    "dictionary_file = 'WiDS Datathon 2020 Dictionary.csv'\n",
    "test_file = 'unlabeled.csv'\n",
    "\n",
    "print('*************************Training Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "data = load_file(data_file)\n",
    "print('Training data shape: {}'.format(data.shape))\n",
    "\n",
    "print('\\nDownsampling training data...')\n",
    "new_data = downsample_data(data)\n",
    "print('Downsampled data shape: {}'.format(new_data.shape))\n",
    "\n",
    "print('\\nSplitting training data into feature variables and target variable...')\n",
    "train_X, train_y = feature_target_split(new_data, 'hospital_death')\n",
    "print('Training feature data shape: {}'.format(train_X.shape))\n",
    "print('Training target data shape: {}'.format(train_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "# Columns to be dropped based on manual evaluation\n",
    "feature_list = ['encounter_id', 'patient_id', 'hospital_id', 'icu_id', 'height', 'weight', 'readmission_status']\n",
    "# Columns to be dropped that have greater than 75% of their data missing\n",
    "columns = train_X.columns[train_X.isnull().mean() > 0.75].tolist()\n",
    "for column in columns:\n",
    "    feature_list.append(column)\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "column_majority_train_X = drop_columns(train_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(column_majority_train_X.shape))\n",
    "\n",
    "print('\\nDealing with missing values - ROW WISE...')\n",
    "# Rows to be dropped that have greater than 50% of their data missing\n",
    "idx1 = column_majority_train_X.index[column_majority_train_X.isnull().mean(axis=1) > 0.5].tolist()\n",
    "print('Dropping {} rows'.format(len(idx1)))\n",
    "clean_train_X = drop_rows(column_majority_train_X, idx1)\n",
    "print('Training feature data shape: {}'.format(clean_train_X.shape))\n",
    "clean_train_y = drop_rows(train_y, idx1)\n",
    "print('Training target data shape: {}'.format(clean_train_y.shape))\n",
    "\n",
    "print('\\nTransforming features...')\n",
    "clean_train_X['bmi'] = clean_train_X['bmi'].apply(BMI_to_cat)\n",
    "clean_train_X['pre_icu_los_days'] = clean_train_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "cat_col, num_col = num_cat_col(clean_train_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_train_X[cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_train_X[num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "cat_list = ['hospital_admit_source', 'apache_2_bodysystem', 'gcs_unable_apache']\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_cat = drop_columns(clean_train_X[cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_cat_imputed = impute_cat_col(X_cat)\n",
    "# Obtaining the index values of rows where arf_apache is null and then dropping them\n",
    "idx2 = X_cat_imputed.index[X_cat_imputed['arf_apache'].isnull()].tolist()\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_cat_final = drop_rows(X_cat_imputed, idx2)\n",
    "print('Categorical data shape: {}'.format(X_cat_final.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "X_num = drop_rows(clean_train_X[num_col], idx2)\n",
    "num_column_list = X_num.columns[X_num.isnull().mean() > 0.50].tolist()\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_num = drop_columns(X_num, num_column_list)\n",
    "X_num = impute_num_col(X_num)\n",
    "print('Numerical data shape: {}'.format(X_num.shape))\n",
    "\n",
    "print('\\nCleaning up the target variable...')\n",
    "# Dropping rows where arf_apache is null (obtained from clean_cat_col function)\n",
    "print('Dropping {} rows'.format(len(idx2)))\n",
    "final_train_y = drop_rows(clean_train_y, idx2)\n",
    "print('Training target shape: {}'.format(final_train_y.shape))\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_cat_encoded = one_hot_encode(X_cat_final)\n",
    "print('Categorcal data shape: {}'.format(X_cat_encoded.shape))\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "new_dataset = concat_num_cat(X_num, X_cat_encoded)\n",
    "print('New training feature dataset shape: {}'.format(new_dataset.shape))\n",
    "\n",
    "\n",
    "print('\\n\\n*************************Test Data*************************')\n",
    "\n",
    "print('\\nLoading data...')\n",
    "test_data = load_file(test_file)\n",
    "print('Test data shape: {}'.format(test_data.shape))\n",
    "\n",
    "print('\\nSplitting test data into feature variables and target variable...')\n",
    "test_X, test_y = feature_target_split(test_data, 'hospital_death')\n",
    "print('Test feature data shape: {}'.format(test_X.shape))\n",
    "print('Test target data shape: {}'.format(test_y.shape))\n",
    "\n",
    "print('\\nDealing with missing values - COLUMN WISE...')\n",
    "print('Dropping {} columns'.format(len(feature_list)))\n",
    "clean_test_X = drop_columns(test_X, feature_list)\n",
    "print('Training feature data shape: {}'.format(clean_test_X.shape))\n",
    "\n",
    "print('\\nTransforming features...')\n",
    "clean_test_X['bmi'] = clean_test_X['bmi'].apply(BMI_to_cat)\n",
    "clean_test_X['pre_icu_los_days'] = clean_test_X['pre_icu_los_days'].apply(PILD_to_cat)\n",
    "\n",
    "print('\\nSplitting categorical variables and numerical variables...')\n",
    "test_cat_col, test_num_col = num_cat_col(clean_test_X, dictionary_file)\n",
    "print('Categorical data shape: {}'.format(clean_test_X[test_cat_col].shape))\n",
    "print('Numerical data shape: {}'.format(clean_test_X[test_num_col].shape))\n",
    "\n",
    "print('\\nCleaning up categorical variables...')\n",
    "# Dropping columns hospital_admit_source, apache_2_bodysystem, gcs_unable_apache\n",
    "print('Dropping {} columns'.format(len(cat_list)))\n",
    "X_test_cat = drop_columns(clean_test_X[test_cat_col], cat_list)\n",
    "# Imputing missing values based on evaluation of data\n",
    "X_test_cat_imputed = impute_cat_col(X_test_cat)\n",
    "\n",
    "# Imputing values instead of dropping rows\n",
    "np.random.seed(0)\n",
    "my_list = ['arf_apache', 'intubated_apache', 'ventilated_apache', 'aids', 'cirrhosis', \n",
    "            'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', \n",
    "            'solid_tumor_with_metastasis']\n",
    "for my_column in my_list:\n",
    "    keys_list = X_test_cat_imputed[my_column].value_counts().keys().tolist()\n",
    "    prob = X_test_cat_imputed[my_column].value_counts(normalize=True).tolist()\n",
    "    X_test_cat_imputed.loc[:, my_column] = X_test_cat_imputed[my_column].fillna(\n",
    "        pd.Series(np.random.choice(keys_list, p=prob, size=len(X_test_cat_imputed))))\n",
    "    \n",
    "print('Categorical data shape: {}'.format(X_test_cat_imputed.shape))\n",
    "\n",
    "print('\\nCleaning up numerical variables...')\n",
    "print('Dropping {} columns'.format(len(num_column_list)))\n",
    "X_test_num = drop_columns(clean_test_X[test_num_col], num_column_list)\n",
    "X_test_num = impute_num_col(X_test_num)\n",
    "print('Numerical data shape: {}'.format(X_test_num.shape))\n",
    "\n",
    "print('\\nOne Hot Encoding the categorical variables...')\n",
    "X_test_cat_encoded = one_hot_encode(X_test_cat_imputed)\n",
    "print('Categorcal data shape: {}'.format(X_test_cat_encoded.shape))\n",
    "\n",
    "print('\\nConcatenating the numerical and categorical variables...')\n",
    "new_test_dataset = concat_num_cat(X_test_num, X_test_cat_encoded)\n",
    "print('New training feature dataset shape: {}'.format(new_test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
